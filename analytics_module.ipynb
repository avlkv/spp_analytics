{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c010bfd-1789-4fd7-9e5c-2c940aa2df39",
   "metadata": {},
   "source": [
    "# Задача\n",
    "В соответствии с предпочтениями конкретой роли оценить документы с помощью модели с конкретными настройками для этой роли, которая привязана к этим предпочтениям.\n",
    "1. Отобрать документы, которые еще не были оценены этой моделью с этими настройками.\n",
    "2. Пройтись по всем документам и оценить их с помощью этой модели. Результат положить в spp_model_score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eb8fd0-b9ac-4436-8931-0c7e38f1303d",
   "metadata": {},
   "source": [
    "# Требования?\n",
    "1. У .py модели должно быть особенное название?\n",
    "\n",
    "# Принцип работы\n",
    "1. Запускается новый плагин, который работает с уже загруженными документами. В этом плагине обязательно описывается конфигурация:\n",
    "- Модуль Analyze для обработки текстов в соответствии с заданными предпочтениями (указывается название роли). Плагин проходит по всем ролям и оценивает еще не оцененные для этой роли документы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4032d69-863c-4a76-bab3-22f32240c9c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f079f2e1-0e0d-4162-8fab-4bd1540018d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.spp.task.bus import Bus\n",
    "from src.spp.task.module.spp_module import SPP_module\n",
    "\n",
    "\n",
    "class Analyze(SPP_module):\n",
    "    \"\"\"\n",
    "    Модуль аналитики\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bus: Bus):\n",
    "        super().__init__(bus, 'Analyze') # Делает Bus = Bus и инициализирует логирование модуля\n",
    "        \n",
    "        # В шину нужно положить необходимые для анализа документы\n",
    "\n",
    "        # Итерация по документам в шине, у которых нужно провести анализ текста (предсказать интересность)\n",
    "        for doc in self.bus.documents.data:\n",
    "            if isinstance(doc.text, str):\n",
    "                prediction = self.predict(doc.text) # json-результат предсказания\n",
    "                # prediction должен быть загружен в таблицу spp_model_score\n",
    "\n",
    "    @staticmethod\n",
    "    def predict(text: str) -> str:\n",
    "        \"\"\"\n",
    "        Метод удаляет\n",
    "        \"\"\"\n",
    "        return re.sub(pattern, ' ', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449e0d5d-cece-47d8-b9a8-9f62955b63d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34822061-83ff-4e69-be3f-e5a6e0121560",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Файл модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6343bf15-bc01-4429-8694-007d58d48460",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Парсер плагина SPP\n",
    "\n",
    "1/2 документ плагина\n",
    "\"\"\"\n",
    "\n",
    "#\n",
    "\n",
    "import logging\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from src.spp.types import SPP_document\n",
    "\n",
    "\n",
    "class COUNTER_MODEL:\n",
    "   \n",
    "\n",
    "    def __init__(self, date_begin: datetime.datetime = datetime.datetime(2022, 7, 5), *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Конструктор класса модели\n",
    "        \"\"\"\n",
    "        # Обнуление списка\n",
    "        self._content_document = []\n",
    "        self.DATE_BEGIN = date_begin\n",
    "\n",
    "        # Логер должен подключаться так. Вся настройка лежит на платформе\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.debug(f\"Model class init completed\")\n",
    "        # self.logger.info(f\"Set source: {self.SOURCE_NAME}\")\n",
    "        ...\n",
    "\n",
    "    def content(self) -> list[SPP_document]:\n",
    "        \"\"\"\n",
    "        Главный метод модели. Его будет вызывать платформа. Он вызывает метод _predict и возвращает список документов\n",
    "        :return:\n",
    "        :rtype:\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"Prediction process start\")\n",
    "        self._parse()\n",
    "        self.logger.debug(\"Prediction process finished\")\n",
    "        return self._content_document\n",
    "\n",
    "    def _parse(self):\n",
    "        \"\"\"\n",
    "        Метод, занимающийся парсингом. Он добавляет в _content_document документы, которые получилось обработать\n",
    "        :return:\n",
    "        :rtype:\n",
    "        \"\"\"\n",
    "        # HOST - это главная ссылка на источник, по которому будет \"бегать\" парсер\n",
    "        self.logger.debug(F\"Parser enter to {self.HOST}\")\n",
    "\n",
    "        # ========================================\n",
    "        # Тут должен находится блок кода, отвечающий за парсинг конкретного источника\n",
    "        # -\n",
    "        urls = []\n",
    "\n",
    "        for i in range(len(self.TAGS)):\n",
    "            splitter = \"<option value=|>\"\n",
    "            massive_s = re.split(splitter, self.TAGS[i])\n",
    "            number = massive_s[1]\n",
    "            url = 'https://www.nist.gov/news-events/news/search?k=&t='\n",
    "            req = requests.get(url + number)\n",
    "            if req.status_code == 200:\n",
    "                req.encoding = \"UTF-8\"\n",
    "                soup = BeautifulSoup(req.content.decode('utf-8'), 'html.parser')\n",
    "                try:\n",
    "                    last_page = int(\n",
    "                        soup.find('li', class_=\"pager__item pager__item--last\").find('a')['href'].split('page')[1][1::])\n",
    "                except AttributeError:\n",
    "                    last_page = 0\n",
    "                for j in range(last_page + 1):\n",
    "                    req = requests.get(url + number + \"&page=\" + str(j))\n",
    "                    req.encoding = \"UTF-8\"\n",
    "                    soup = BeautifulSoup(req.content.decode('utf-8'), 'html.parser')\n",
    "                    articles = soup.find_all('div', class_=\"nist-teaser__content-wrapper\")\n",
    "                    for article in articles:\n",
    "\n",
    "                        news_date = datetime.datetime.strptime(article.find('time').text, '%B %d, %Y')\n",
    "\n",
    "                        # Проверяем, что новость не ранее 01.01.2019\n",
    "                        if news_date > self.DATE_BEGIN:\n",
    "                            # print(article.find('a')['href'])\n",
    "                            urls.append(\"https://www.nist.gov\" + article.find('a')['href'])\n",
    "                            # print(article.find('a')['href'])\n",
    "                    # print(url + number + \"&page=\" + str(j))\n",
    "            else:\n",
    "                # logger.error('Ошибка загрузки')\n",
    "                ...\n",
    "\n",
    "        new_urls = []\n",
    "        for i in range(len(urls)):\n",
    "            if not urls[i] in new_urls:\n",
    "                # print(urls[i])\n",
    "                new_urls.append(urls[i])\n",
    "\n",
    "        for ref in (range(len(new_urls))):\n",
    "            web_link = new_urls[ref]\n",
    "            title, load_date, s_text, pub_date_text = self._document_parse(web_link)\n",
    "\n",
    "            document = SPP_document(\n",
    "                doc_id=None,\n",
    "                title=title,\n",
    "                abstract=None,\n",
    "                text=s_text,\n",
    "                web_link=web_link,\n",
    "                local_link=None,\n",
    "                other_data={},\n",
    "                pub_date=dateutil.parser.isoparse(pub_date_text),\n",
    "                load_date=load_date\n",
    "            )\n",
    "\n",
    "            # Логирование найденного документа\n",
    "            self.logger.info(self._find_document_text_for_logger(document))\n",
    "\n",
    "            self._content_document.append(document)\n",
    "            time.sleep(1)\n",
    "        # ---\n",
    "        # ========================================\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    def _find_document_text_for_logger(doc: SPP_document):\n",
    "        \"\"\"\n",
    "        Единый для всех парсеров метод, который подготовит на основе SPP_document строку для логера\n",
    "        :param doc: Документ, полученный парсером во время своей работы\n",
    "        :type doc:\n",
    "        :return: Строка для логера на основе документа\n",
    "        :rtype:\n",
    "        \"\"\"\n",
    "        return f\"Find document | name: {doc.title} | link to web: {doc.web_link} | publication date: {doc.pub_date}\"\n",
    "\n",
    "    def _document_parse(self, ref):\n",
    "        \"\"\"\n",
    "        Метод для непосредственного парсинга важных данных документа по ссылке\n",
    "        :param ref:\n",
    "        :type ref:\n",
    "        :return:\n",
    "        :rtype:\n",
    "        \"\"\"\n",
    "        try:\n",
    "            ufr = requests.get(ref)  # делаем запрос\n",
    "            ufr.encoding = 'utf-8'\n",
    "\n",
    "            if ufr.status_code == 200:\n",
    "                idk = ref.split(\"/\")\n",
    "                f_name = idk[-1]\n",
    "                load_date = datetime.datetime.now()\n",
    "\n",
    "                soup = BeautifulSoup(ufr.content.decode('utf-8'), 'html.parser')\n",
    "                s_text = \"\"\n",
    "                for j in soup.find_all(\"div\", class_=\"text-with-summary\"):\n",
    "                    for link2 in j.find_all(\"p\"):\n",
    "                        s_text = s_text + link2.text\n",
    "                s_text = s_text.replace('\\n', ' ').replace('\\t', ' ').replace(\"¶\", \" \").replace(\"▲\", \" \").replace(\n",
    "                    '\\xa0', ' ').replace('\\r', ' ').replace('—', \"-\").replace(\"’\", \"'\").replace(\"“\", '\"').replace(\"”\",\n",
    "                                                                                                                  '\"').replace(\n",
    "                    \" \", \" \")\n",
    "                while '  ' in s_text:\n",
    "                    s_text = s_text.replace('  ', ' ')\n",
    "\n",
    "                div_datetime = soup.find(\"div\", class_='font-heading-md')\n",
    "                pub_date = div_datetime.find('time').attrs['datetime']\n",
    "\n",
    "                return f_name, load_date, s_text, pub_date\n",
    "            else:\n",
    "                self.logger.debug(f'Document processing error. Returned status code {ufr.status_code}')\n",
    "        except Exception as e:\n",
    "            self.logger.debug(f'Document processing error. Exception {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f03d3c-1a46-425d-b017-1205fef02dbc",
   "metadata": {},
   "source": [
    "## Конфигурация модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefcb2eb-4983-41d2-80b6-7481cd5c0bf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ef4539e-4dc2-44ca-a0c9-9858f76a3e81",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Исходники\n",
    "https://github.com/CuberHuber/NSPK-DI-SPP-plugin-nist/blob/main/nist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f44654-103e-4850-8215-257b93e3863a",
   "metadata": {},
   "source": [
    "## Файл парсера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911d5840-0403-429e-ad18-a9ca5fb8e0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Парсер плагина SPP\n",
    "\n",
    "1/2 документ плагина\n",
    "\"\"\"\n",
    "import logging\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from src.spp.types import SPP_document\n",
    "\n",
    "\n",
    "class NIST:\n",
    "    \"\"\"\n",
    "    Класс парсера плагина SPP\n",
    "\n",
    "    :source: nist\n",
    "    :link: https://www.nist.gov/news-events/news\n",
    "\n",
    "    :warning Все необходимое для работы парсера должно находится внутри этого класса\n",
    "\n",
    "    :_content_document: Это список объектов документа. При старте класса этот список должен обнулиться,\n",
    "                        а затем по мере обработки источника - заполняться.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    HOST: str = 'https://www.nist.gov/news-events/news'\n",
    "    SOURCE_NAME: str = 'nist'\n",
    "\n",
    "    TAGS: list[str] = [\n",
    "        \"<option value=249466>Advanced communications</option>\",\n",
    "        \"<option value=248311>-Quantum communications</option>\",\n",
    "        # \"<option value=248316>-Wireless (RF)</option>\",\n",
    "        # \"<option value=248376>-Building codes and standards</option>\",\n",
    "        # \"<option value=248381>-Building control systems</option>\",\n",
    "        # \"<option value=248486>Electronics</option>\",\n",
    "        # \"<option value=248491>-Electromagnetics</option>\",\n",
    "        # \"<option value=248526>-Flexible electronics</option>\",\n",
    "        # \"<option value=248496>-Magnetoelectronics</option>\",\n",
    "        # \"<option value=248501>-Optoelectronics</option>\",\n",
    "        # \"<option value=249491>-Organic electronics</option>\",\n",
    "        # \"<option value=248511>-Semiconductors</option>\",\n",
    "        # \"<option value=248516>-Sensors</option>\",\n",
    "        # \"<option value=248521>-Superconducting electronics</option>\",\n",
    "        # \"<option value=249421>Information technology</option>\",\n",
    "        # \"<option value=2753736>-Artificial intelligence</option>\",\n",
    "        # \"<option value=2800826>--AI measurement and evaluation</option>\",\n",
    "        # \"<option value=2788806>--Applied AI</option>\",\n",
    "        # \"<option value=2788801>--Fundamental AI</option>\",\n",
    "        # \"<option value=2800831>--Hardware for AI</option>\",\n",
    "        # \"<option value=2800991>--Machine learning</option>\",\n",
    "        # \"<option value=2800836>--Trustworthy and responsible AI</option>\",\n",
    "        # \"<option value=248701>-Biometrics</option>\",\n",
    "        # \"<option value=248706>-Cloud computing and virtualization</option>\",\n",
    "        # \"<option value=248711>-Complex systems</option>\",\n",
    "        # \"<option value=248716>-Computational science</option>\",\n",
    "        # \"<option value=248721>-Conformance testing</option>\",\n",
    "        # \"<option value=248726>-Cyber-physical systems</option>\",\n",
    "        # \"<option value=2746861>--Smart cities</option>\",\n",
    "        # \"<option value=248731>-Cybersecurity</option>\",\n",
    "        # \"<option value=248746>--Cryptography</option>\",\n",
    "        # \"<option value=2753741>--Cybersecurity education and workforce development</option>\",\n",
    "        # \"<option value=2788811>--Cybersecurity measurement</option>\",\n",
    "        # \"<option value=248736>--Identity and access management</option>\",\n",
    "        # \"<option value=2788816>--Privacy engineering</option>\",\n",
    "        # \"<option value=248751>--Risk management</option>\",\n",
    "        # \"<option value=2788821>--Securing emerging technologies</option>\",\n",
    "        # \"<option value=2788826>--Trustworthy networks</option>\",\n",
    "        # \"<option value=2788831>--Trustworthy platforms</option>\",\n",
    "        # \"<option value=248756>-Data and informatics</option>\",\n",
    "        # \"<option value=248761>--Human language technology</option>\",\n",
    "        # \"<option value=248766>--Information retrieval</option>\",\n",
    "        # \"<option value=248781>-Federal information processing standards (FIPS)</option>\",\n",
    "        # \"<option value=248786>-Health IT</option>\",\n",
    "        # \"<option value=2748441>-Internet of Things (IoT)</option>\",\n",
    "        # \"<option value=248796>-Interoperability testing</option>\",\n",
    "        # \"<option value=248801>-Mobile</option>\",\n",
    "        # \"<option value=248806>-Networking</option>\",\n",
    "        # \"<option value=2753766>--Mobile and wireless networking</option>\",\n",
    "        # \"<option value=2753756>--Network management and monitoring</option>\",\n",
    "        # \"<option value=248811>--Network modeling and analysis</option>\",\n",
    "        # \"<option value=248771>--Natural language processing</option>\",\n",
    "        # \"<option value=2753746>--Network security and robustness</option>\",\n",
    "        # \"<option value=2753751>--Network test and measurement</option>\",\n",
    "        # \"<option value=248816>--Next generation networks</option>\",\n",
    "        # \"<option value=2753761>--Protocol&nbsp;design and standardization</option>\",\n",
    "        # \"<option value=2753771>--Software defined and virtual networks</option>\",\n",
    "        # \"<option value=248821>-Privacy</option>\",\n",
    "        # \"<option value=248826>-Software research</option>\",\n",
    "        # \"<option value=248841>--Software testing</option>\",\n",
    "        # \"<option value=248846>-Usability and human factors</option>\",\n",
    "        # \"<option value=248851>--Accessibility</option>\",\n",
    "        # \"<option value=248776>-Video analytics</option>\",\n",
    "        # \"<option value=2788836>-Virtual / augmented reality</option>\",\n",
    "        # \"<option value=248856>-Visualization research</option>\",\n",
    "        # \"<option value=248861>-Voting systems</option>\",\n",
    "        # \"<option value=2748236>Infrastructure</option>\",\n",
    "        # \"<option value=248936>-Technology commercialization</option>\",\n",
    "        # \"<option value=249011>Mathematics and statistics</option>\",\n",
    "        # \"<option value=249016>-Experiment design</option>\",\n",
    "        # \"<option value=249021>-Image and signal processing</option>\",\n",
    "        # \"<option value=249031>-Modeling and simulation research</option>\",\n",
    "        # \"<option value=249036>-Numerical methods and software</option>\",\n",
    "        # \"<option value=249041>-Statistical analysis</option>\",\n",
    "        # \"<option value=249046>-Uncertainty quantification</option>\",\n",
    "        # \"<option value=249341>Standards</option>\",\n",
    "        # \"<option value=249346>-Accreditation</option>\",\n",
    "        # \"<option value=249416>-Calibration services</option>\",\n",
    "        # \"<option value=249366>-Conformity assessment</option>\",\n",
    "        # \"<option value=249371>-Documentary standards</option>\",\n",
    "        # \"<option value=2748516>-Frameworks</option>\",\n",
    "        # \"<option value=249391>-Standards education</option>\",\n",
    "    ]\n",
    "    DATE_BEGIN: datetime.datetime\n",
    "\n",
    "    _content_document: list[SPP_document]\n",
    "\n",
    "    def __init__(self, date_begin: datetime.datetime = datetime.datetime(2022, 7, 5), *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Конструктор класса парсера\n",
    "        \"\"\"\n",
    "        # Обнуление списка\n",
    "        self._content_document = []\n",
    "        self.DATE_BEGIN = date_begin\n",
    "\n",
    "        # Логер должен подключаться так. Вся настройка лежит на платформе\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.debug(f\"Parser class init completed\")\n",
    "        self.logger.info(f\"Set source: {self.SOURCE_NAME}\")\n",
    "        ...\n",
    "\n",
    "    def content(self) -> list[SPP_document]:\n",
    "        \"\"\"\n",
    "        Главный метод парсера. Его будет вызывать платформа. Он вызывает метод _parse и возвращает список документов\n",
    "        :return:\n",
    "        :rtype:\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"Parse process start\")\n",
    "        self._parse()\n",
    "        self.logger.debug(\"Parse process finished\")\n",
    "        return self._content_document\n",
    "\n",
    "    def _parse(self):\n",
    "        \"\"\"\n",
    "        Метод, занимающийся парсингом. Он добавляет в _content_document документы, которые получилось обработать\n",
    "        :return:\n",
    "        :rtype:\n",
    "        \"\"\"\n",
    "        # HOST - это главная ссылка на источник, по которому будет \"бегать\" парсер\n",
    "        self.logger.debug(F\"Parser enter to {self.HOST}\")\n",
    "\n",
    "        # ========================================\n",
    "        # Тут должен находится блок кода, отвечающий за парсинг конкретного источника\n",
    "        # -\n",
    "        urls = []\n",
    "\n",
    "        for i in range(len(self.TAGS)):\n",
    "            splitter = \"<option value=|>\"\n",
    "            massive_s = re.split(splitter, self.TAGS[i])\n",
    "            number = massive_s[1]\n",
    "            url = 'https://www.nist.gov/news-events/news/search?k=&t='\n",
    "            req = requests.get(url + number)\n",
    "            if req.status_code == 200:\n",
    "                req.encoding = \"UTF-8\"\n",
    "                soup = BeautifulSoup(req.content.decode('utf-8'), 'html.parser')\n",
    "                try:\n",
    "                    last_page = int(\n",
    "                        soup.find('li', class_=\"pager__item pager__item--last\").find('a')['href'].split('page')[1][1::])\n",
    "                except AttributeError:\n",
    "                    last_page = 0\n",
    "                for j in range(last_page + 1):\n",
    "                    req = requests.get(url + number + \"&page=\" + str(j))\n",
    "                    req.encoding = \"UTF-8\"\n",
    "                    soup = BeautifulSoup(req.content.decode('utf-8'), 'html.parser')\n",
    "                    articles = soup.find_all('div', class_=\"nist-teaser__content-wrapper\")\n",
    "                    for article in articles:\n",
    "\n",
    "                        news_date = datetime.datetime.strptime(article.find('time').text, '%B %d, %Y')\n",
    "\n",
    "                        # Проверяем, что новость не ранее 01.01.2019\n",
    "                        if news_date > self.DATE_BEGIN:\n",
    "                            # print(article.find('a')['href'])\n",
    "                            urls.append(\"https://www.nist.gov\" + article.find('a')['href'])\n",
    "                            # print(article.find('a')['href'])\n",
    "                    # print(url + number + \"&page=\" + str(j))\n",
    "            else:\n",
    "                # logger.error('Ошибка загрузки')\n",
    "                ...\n",
    "\n",
    "        new_urls = []\n",
    "        for i in range(len(urls)):\n",
    "            if not urls[i] in new_urls:\n",
    "                # print(urls[i])\n",
    "                new_urls.append(urls[i])\n",
    "\n",
    "        for ref in (range(len(new_urls))):\n",
    "            web_link = new_urls[ref]\n",
    "            title, load_date, s_text, pub_date_text = self._document_parse(web_link)\n",
    "\n",
    "            document = SPP_document(\n",
    "                doc_id=None,\n",
    "                title=title,\n",
    "                abstract=None,\n",
    "                text=s_text,\n",
    "                web_link=web_link,\n",
    "                local_link=None,\n",
    "                other_data={},\n",
    "                pub_date=dateutil.parser.isoparse(pub_date_text),\n",
    "                load_date=load_date\n",
    "            )\n",
    "\n",
    "            # Логирование найденного документа\n",
    "            self.logger.info(self._find_document_text_for_logger(document))\n",
    "\n",
    "            self._content_document.append(document)\n",
    "            time.sleep(1)\n",
    "        # ---\n",
    "        # ========================================\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    def _find_document_text_for_logger(doc: SPP_document):\n",
    "        \"\"\"\n",
    "        Единый для всех парсеров метод, который подготовит на основе SPP_document строку для логера\n",
    "        :param doc: Документ, полученный парсером во время своей работы\n",
    "        :type doc:\n",
    "        :return: Строка для логера на основе документа\n",
    "        :rtype:\n",
    "        \"\"\"\n",
    "        return f\"Find document | name: {doc.title} | link to web: {doc.web_link} | publication date: {doc.pub_date}\"\n",
    "\n",
    "    def _document_parse(self, ref):\n",
    "        \"\"\"\n",
    "        Метод для непосредственного парсинга важных данных документа по ссылке\n",
    "        :param ref:\n",
    "        :type ref:\n",
    "        :return:\n",
    "        :rtype:\n",
    "        \"\"\"\n",
    "        try:\n",
    "            ufr = requests.get(ref)  # делаем запрос\n",
    "            ufr.encoding = 'utf-8'\n",
    "\n",
    "            if ufr.status_code == 200:\n",
    "                idk = ref.split(\"/\")\n",
    "                f_name = idk[-1]\n",
    "                load_date = datetime.datetime.now()\n",
    "\n",
    "                soup = BeautifulSoup(ufr.content.decode('utf-8'), 'html.parser')\n",
    "                s_text = \"\"\n",
    "                for j in soup.find_all(\"div\", class_=\"text-with-summary\"):\n",
    "                    for link2 in j.find_all(\"p\"):\n",
    "                        s_text = s_text + link2.text\n",
    "                s_text = s_text.replace('\\n', ' ').replace('\\t', ' ').replace(\"¶\", \" \").replace(\"▲\", \" \").replace(\n",
    "                    '\\xa0', ' ').replace('\\r', ' ').replace('—', \"-\").replace(\"’\", \"'\").replace(\"“\", '\"').replace(\"”\",\n",
    "                                                                                                                  '\"').replace(\n",
    "                    \" \", \" \")\n",
    "                while '  ' in s_text:\n",
    "                    s_text = s_text.replace('  ', ' ')\n",
    "\n",
    "                div_datetime = soup.find(\"div\", class_='font-heading-md')\n",
    "                pub_date = div_datetime.find('time').attrs['datetime']\n",
    "\n",
    "                return f_name, load_date, s_text, pub_date\n",
    "            else:\n",
    "                self.logger.debug(f'Document processing error. Returned status code {ufr.status_code}')\n",
    "        except Exception as e:\n",
    "            self.logger.debug(f'Document processing error. Exception {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77ba258-5dcd-453b-9608-fbdbb501fd9e",
   "metadata": {},
   "source": [
    "## Файл SPPFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c847482-f637-44a0-bbfa-08dcfaa33c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This plugin processes Native source\n",
    "# Link: https://www.nist.gov/news-events/news\n",
    "# Author:\n",
    "#\tЛупашко Р., Соловьев И.\n",
    "#\tNSPK DI\n",
    "\n",
    "SOURCE nist\n",
    "\n",
    "# Тут указывается имя файла с парсером\n",
    "PARSER nist\n",
    "\n",
    "SETENV LogMode debug\n",
    "\n",
    "# Тут указывается класс парсера и метод, который платформа будет вызывать\n",
    "START NIST content\n",
    "\n",
    "# Такая конфигурация модулей подходит для \"Нативных источников\"\n",
    "ADD FilterOnlyNewDocumentWithDB\n",
    "ADD UploadDocumentToDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59f9759-d290-4430-aa58-2dee8707a0e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2858dbc6-b765-4b6c-89ab-00496120ed57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
